{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_data = json.load(open('annotations_learn.json','r'))\n",
    "len(ds_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create spaCy DocBin objects from the annotated data\n",
    "def get_spacy_doc(file, data):\n",
    "  # Create a blank spaCy pipeline\n",
    "  nlp = spacy.blank('ru')\n",
    "  db = DocBin()\n",
    "\n",
    "  # Iterate through the data\n",
    "  for text, annot in tqdm(data):\n",
    "    doc = nlp.make_doc(text)\n",
    "    annot = annot['entities']\n",
    "\n",
    "    ents = []\n",
    "    entity_indices = []\n",
    "\n",
    "    # Extract entities from the annotations\n",
    "    for start, end, label in annot:\n",
    "      skip_entity = False\n",
    "      for idx in range(start, end):\n",
    "        if idx in entity_indices:\n",
    "          skip_entity = True\n",
    "          break\n",
    "      if skip_entity:\n",
    "        continue\n",
    "\n",
    "      entity_indices = entity_indices + list(range(start, end))\n",
    "      try:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode='strict')\n",
    "      except:\n",
    "        continue\n",
    "\n",
    "      if span is None:\n",
    "        # Log errors for annotations that couldn't be processed\n",
    "        err_data = str([start, end]) + \"    \" + str(text) + \"\\n\"\n",
    "        file.write(err_data)\n",
    "      else:\n",
    "        ents.append(span)\n",
    "\n",
    "    try:\n",
    "      doc.ents = ents\n",
    "      db.add(doc)\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "  return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(ds_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 689.65it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 597.22it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('trained/train_file.txt', 'w+') as wfile:\n",
    "    db = get_spacy_doc(wfile, train)\n",
    "    db.to_disk('trained/train_data.spacy')\n",
    "    db = get_spacy_doc(wfile, test)\n",
    "    db.to_disk('trained/test_data.spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_utils import BatchEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: trained\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  ------  ------  ------  ------\n",
      "  0       0        2122.45    771.30    0.00    0.00    0.00    0.00\n",
      "100     200       27384.31  44104.22   75.47   73.17   77.92    0.75\n",
      "200     400          24.58  16274.54   79.75   77.78   81.82    0.80\n",
      "300     600          34.09  15720.07   79.25   76.83   81.82    0.79\n",
      "400     800           9.39  15059.79   76.00   78.08   74.03    0.76\n",
      "500    1000           0.00  14364.88   76.00   78.08   74.03    0.76\n",
      "600    1200           0.00  13644.78   77.12   77.63   76.62    0.77\n",
      "700    1400           0.00  12849.41   78.21   77.22   79.22    0.78\n",
      "800    1600           0.00  11913.47   72.60   76.81   68.83    0.73\n",
      "900    1800           0.00  10894.85   76.00   78.08   74.03    0.76\n",
      "1000    2000           0.00   9726.11   76.82   78.38   75.32    0.77\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "trained\\model-last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\n",
      "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<?, ?B/s]\n",
      "c:\\Users\\amatemeow\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\amatemeow\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "\n",
      "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]\n",
      "config.json: 100%|██████████| 625/625 [00:00<?, ?B/s] \n",
      "\n",
      "vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]\n",
      "vocab.txt: 100%|██████████| 872k/872k [00:00<00:00, 1.82MB/s]\n",
      "vocab.txt: 100%|██████████| 872k/872k [00:00<00:00, 1.82MB/s]\n",
      "\n",
      "tokenizer.json:   0%|          | 0.00/1.72M [00:00<?, ?B/s]\n",
      "tokenizer.json: 100%|██████████| 1.72M/1.72M [00:00<00:00, 2.62MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.72M/1.72M [00:00<00:00, 2.61MB/s]\n",
      "\n",
      "model.safetensors:   0%|          | 0.00/672M [00:00<?, ?B/s]\n",
      "model.safetensors:   2%|▏         | 10.5M/672M [00:00<01:00, 11.0MB/s]\n",
      "model.safetensors:   3%|▎         | 21.0M/672M [00:01<00:56, 11.5MB/s]\n",
      "model.safetensors:   5%|▍         | 31.5M/672M [00:02<00:55, 11.6MB/s]\n",
      "model.safetensors:   6%|▌         | 41.9M/672M [00:03<00:53, 11.7MB/s]\n",
      "model.safetensors:   8%|▊         | 52.4M/672M [00:04<00:52, 11.7MB/s]\n",
      "model.safetensors:   9%|▉         | 62.9M/672M [00:05<00:51, 11.8MB/s]\n",
      "model.safetensors:  11%|█         | 73.4M/672M [00:06<00:50, 11.8MB/s]\n",
      "model.safetensors:  12%|█▏        | 83.9M/672M [00:07<00:49, 11.8MB/s]\n",
      "model.safetensors:  14%|█▍        | 94.4M/672M [00:08<00:49, 11.8MB/s]\n",
      "model.safetensors:  16%|█▌        | 105M/672M [00:08<00:48, 11.8MB/s] \n",
      "model.safetensors:  17%|█▋        | 115M/672M [00:09<00:47, 11.8MB/s]\n",
      "model.safetensors:  19%|█▊        | 126M/672M [00:10<00:49, 10.9MB/s]\n",
      "model.safetensors:  20%|██        | 136M/672M [00:11<00:47, 11.2MB/s]\n",
      "model.safetensors:  22%|██▏       | 147M/672M [00:12<00:46, 11.3MB/s]\n",
      "model.safetensors:  23%|██▎       | 157M/672M [00:13<00:44, 11.5MB/s]\n",
      "model.safetensors:  25%|██▍       | 168M/672M [00:14<00:43, 11.6MB/s]\n",
      "model.safetensors:  27%|██▋       | 178M/672M [00:15<00:42, 11.7MB/s]\n",
      "model.safetensors:  28%|██▊       | 189M/672M [00:16<00:41, 11.7MB/s]\n",
      "model.safetensors:  30%|██▉       | 199M/672M [00:17<00:40, 11.7MB/s]\n",
      "model.safetensors:  31%|███       | 210M/672M [00:18<00:39, 11.8MB/s]\n",
      "model.safetensors:  33%|███▎      | 220M/672M [00:18<00:38, 11.8MB/s]\n",
      "model.safetensors:  34%|███▍      | 231M/672M [00:19<00:37, 11.8MB/s]\n",
      "model.safetensors:  36%|███▌      | 241M/672M [00:20<00:39, 10.9MB/s]\n",
      "model.safetensors:  37%|███▋      | 252M/672M [00:21<00:37, 11.2MB/s]\n",
      "model.safetensors:  39%|███▉      | 262M/672M [00:22<00:36, 11.4MB/s]\n",
      "model.safetensors:  41%|████      | 273M/672M [00:23<00:34, 11.5MB/s]\n",
      "model.safetensors:  42%|████▏     | 283M/672M [00:24<00:33, 11.6MB/s]\n",
      "model.safetensors:  44%|████▎     | 294M/672M [00:25<00:32, 11.7MB/s]\n",
      "model.safetensors:  45%|████▌     | 304M/672M [00:26<00:31, 11.7MB/s]\n",
      "model.safetensors:  47%|████▋     | 315M/672M [00:27<00:30, 11.7MB/s]\n",
      "model.safetensors:  48%|████▊     | 325M/672M [00:28<00:29, 11.7MB/s]\n",
      "model.safetensors:  50%|████▉     | 336M/672M [00:28<00:28, 11.8MB/s]\n",
      "model.safetensors:  51%|█████▏    | 346M/672M [00:29<00:27, 11.7MB/s]\n",
      "model.safetensors:  53%|█████▎    | 357M/672M [00:30<00:28, 10.9MB/s]\n",
      "model.safetensors:  55%|█████▍    | 367M/672M [00:31<00:27, 11.2MB/s]\n",
      "model.safetensors:  56%|█████▌    | 377M/672M [00:32<00:25, 11.3MB/s]\n",
      "model.safetensors:  58%|█████▊    | 388M/672M [00:33<00:24, 11.5MB/s]\n",
      "model.safetensors:  59%|█████▉    | 398M/672M [00:34<00:23, 11.6MB/s]\n",
      "model.safetensors:  61%|██████    | 409M/672M [00:35<00:22, 11.6MB/s]\n",
      "model.safetensors:  62%|██████▏   | 419M/672M [00:36<00:21, 11.7MB/s]\n",
      "model.safetensors:  64%|██████▍   | 430M/672M [00:37<00:20, 11.7MB/s]\n",
      "model.safetensors:  66%|██████▌   | 440M/672M [00:38<00:19, 11.8MB/s]\n",
      "model.safetensors:  67%|██████▋   | 451M/672M [00:38<00:18, 11.8MB/s]\n",
      "model.safetensors:  69%|██████▊   | 461M/672M [00:39<00:17, 11.8MB/s]\n",
      "model.safetensors:  70%|███████   | 472M/672M [00:40<00:16, 11.8MB/s]\n",
      "model.safetensors:  72%|███████▏  | 482M/672M [00:41<00:17, 10.9MB/s]\n",
      "model.safetensors:  73%|███████▎  | 493M/672M [00:42<00:16, 11.2MB/s]\n",
      "model.safetensors:  75%|███████▍  | 503M/672M [00:43<00:14, 11.4MB/s]\n",
      "model.safetensors:  76%|███████▋  | 514M/672M [00:44<00:13, 11.5MB/s]\n",
      "model.safetensors:  78%|███████▊  | 524M/672M [00:45<00:12, 11.6MB/s]\n",
      "model.safetensors:  80%|███████▉  | 535M/672M [00:46<00:11, 11.7MB/s]\n",
      "model.safetensors:  81%|████████  | 545M/672M [00:47<00:10, 11.7MB/s]\n",
      "model.safetensors:  83%|████████▎ | 556M/672M [00:48<00:09, 11.7MB/s]\n",
      "model.safetensors:  84%|████████▍ | 566M/672M [00:48<00:09, 11.8MB/s]\n",
      "model.safetensors:  86%|████████▌ | 577M/672M [00:49<00:08, 11.7MB/s]\n",
      "model.safetensors:  87%|████████▋ | 587M/672M [00:50<00:07, 11.8MB/s]\n",
      "model.safetensors:  89%|████████▉ | 598M/672M [00:51<00:06, 10.9MB/s]\n",
      "model.safetensors:  90%|█████████ | 608M/672M [00:52<00:05, 11.2MB/s]\n",
      "model.safetensors:  92%|█████████▏| 619M/672M [00:53<00:04, 11.4MB/s]\n",
      "model.safetensors:  94%|█████████▎| 629M/672M [00:54<00:03, 11.5MB/s]\n",
      "model.safetensors:  95%|█████████▌| 640M/672M [00:55<00:02, 11.6MB/s]\n",
      "model.safetensors:  97%|█████████▋| 650M/672M [00:56<00:01, 11.7MB/s]\n",
      "model.safetensors:  98%|█████████▊| 661M/672M [00:57<00:00, 11.7MB/s]\n",
      "model.safetensors: 100%|█████████▉| 671M/672M [00:58<00:00, 11.7MB/s]\n",
      "model.safetensors: 100%|██████████| 672M/672M [00:58<00:00, 11.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train config.cfg --paths.train trained/train_data.spacy  --paths.dev trained/test_data.spacy --gpu-id 0 --output trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
